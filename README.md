# **Classification des Propositions pour la M√©tropole du Grand Paris**

## **Contexte**
Dans le cadre de la campagne *"Construisons la M√©tropole du Grand Paris"*, la **Direction de la D√©mocratie, des Citoyens et des Territoires (DDCT)** de la Ville de Paris a sollicit√© la participation des citoyens et collectivit√©s. Ces derniers ont `soumis des id√©es innovantes` pour transformer la m√©tropole en un espace plus connect√© durable et inclusif. 

Ces propositions d'id√©es ont √©te regroup√©es en **5 grandes th√©matiques**:

1. **Transition √©cologique et Mobilit√©s**
2. **Culture et Identit√©**
3. **Logement et Am√©nagement**
4. **Rayonnement**
5. **Lutte contre les in√©galit√©s**

### **Jeu de donn√©es**
Les propositions ont √©t√© collect√©es dans un jeu de donn√©es comprenant **362 propositions** et **21 attributs**. Parmi ces attributs on trouve :
- **Date de publication**
- **Objectif** (r√©sultats attendus)
- **Description textuelle** (d√©tails des proposition


# **Objectifs du Projet**
L‚Äôobjectif est d‚Äôappliquer une **classification automatique** sur le contenu des id√©es afin d‚Äôidentifier les termes les plus repr√©sentatifs pour chaque th√©matique. 


## **Outils utilis√©s**
- **PySpark** est une interface Python d'Apache Spark un framework open-source con√ßu pour le traitement distribu√© de grandes quantit√©s de donn√©es. **Notre objectifs** :  
   - Manipuler et analyser des donn√©es √† l'aide de Spark SQL et des DataFrames.  
   - Appliquer des algorithmes de machine learning via la biblioth√®que MLlib.  

- **Spark NLP**  
Biblioth√®que de traitement automatique du langage naturel (NLP) bas√©e sur Spark et pour optimis√©e pour les pipelines scalables et le traitement parall√®le.  **Notre objectifs** :  
   - Pr√©traiter et analyser des textes (tokenisation, lemmatisation, annotation).  
   - Construire des pipelines NLP performants et adapt√©s aux grandes volum√©tries.  
   - Exploiter des mod√®les NLP pr√©-entra√Æn√©s pour des t√¢ches sp√©cifiques.  

---------------------


# üìà **√âtapes Cl√©s du Projet**

### **1Ô∏è‚É£ Pr√©traitement des donn√©es textuelles**
Pour pr√©parer les donn√©es textuelles √† la vectorisation nous avons utilis√© la biblioth√®que **Spark NLP** d√©velopp√©e par **John Snow Labs**. Nous avons construit un `pipeline NLP compos√© de plusieurs Annotators` dont le principal objectif est de nettoyer la variable texte avant la vectorisation.  
**Les √©tapes principales incluent** :  

- **Normalisation et Nettoyage** : Uniformiser le texte en transformant les mots en minuscules en supprimant les accents ainsi la ponctuation et les caract√®res sp√©ciaux non pertinents pour l'analyse.  
- **Tokenisation** : D√©coupage du texte en unit√©s de base de mots dans notre cas.  
- **Lemmatisation** : R√©duction des mots √† leur forme canonique ou racine (par exemple, "mangeant" devient "manger").  
- **Correction orthographique (SpellChecker)** : Correction des fautes d'orthographe dans les textes. Nous avons enrichi cette √©tape avec un fichier texte personnalis√© nomm√© `correction_mots` contenant des mots sp√©cifiques √† corriger.  
- **Suppression des Stop Words** : √âlimination des mots courants qui n'apportent pas de valeur s√©mantique significative. Cette √©tape a √©galement √©t√© enrichie avec un fichier texte nomm√© `french`, contenant des mots suppl√©mentaires √† supprimer.

Ces √©tapes de pr√©traitement sont essentielles pour am√©liorer la qualit√© des donn√©es textuelles avant de les soumettre aux mod√®les de vectorisation.

--------------
### 2Ô∏è‚É£ Repr√©sentation Vectorielle

L'objectif ici est de transformer les propositions d'id√©es en donn√©es num√©riques afin de pouvoir les repr√©senter graphiquement ou les analyser avec des mod√®les de machine learning. 
Pour ce faire nous utilisons `les transformeurs` de **Spark NLP** et **Spark MLlib**. Ces outils prennent les donn√©es annot√©es (nettoy√©es) vues pr√©c√©demment et appliquent des transformations pour g√©n√©rer des repr√©sentations vectorielles √† l'aide de mod√®les tels que :

**Mod√®le de Sac de Mots Statistique : TF-IDF avec et sans LSA**

Le mod√®le **TF-IDF** a √©t√© utilis√© pour pond√©rer les termes des propositions mettant en avant les mots significatifs et r√©duisant l'impact des termes fr√©quents. 
Nous avons √©galement test√© **LSA (Latent Semantic Analysis)** qui permet de r√©duire la dimensionnalit√© tout en capturant les relations s√©mantiques entre les termes.

**Mod√®les Bas√©s sur les Embeddings et R√©seaux de Neurones Profonds**

Pour ces mod√®les nous avons utilis√© des **mod√®les pr√©-entra√Æn√©s** sp√©cifiquement con√ßus pour le fran√ßais. Nous avons explor√© deux approches principales :

**Word Embeddings :**
- **Word2Vec** : Utilis√© pour g√©n√©rer des repr√©sentations vectorielles des mots capturant leurs relations de similarit√©.
- **BERT** : Produit des `embeddings contextuels` o√π la repr√©sentation des mots varie selon leur contexte dans une phrase.

**Document/Sentence Embeddings :**
- **Agr√©gation par la moyenne des word embeddings** : Cette m√©thode calcule la moyenne des vecteurs `des mots d'une proposition enti√®re` offrant une repr√©sentation globale.
- **USE (Universal Sentence Encoder)** : G√©n√®re des embeddings pour des phrases enti√®res capturant ainsi le sens global d'une proposition.

------------------------------

### 3Ô∏è‚É£ Application d'une classification automatique et √âvaluation des Repr√©sentations Vectorielles

PySpark propose plusieurs m√©thodes de classification automatique supervis√©es et non supervis√©es (clustering). Dans ce projet nous avons choisi l‚Äôalgorithme K-means pour sa simplicit√© d‚Äôimpl√©mentation et son efficacit√©. De plus PySpark utilise une version `parall√®le de K-means' optimis√©e pour exploiter pleinement les capacit√©s de traitement distribu√©. 

**L‚Äôobjectif principal de cette partie est d‚Äô√©valuer quelle repr√©sentation vectorielle produit les clusters les plus coh√©rents et significatifs. Pour ce faire** :  

**3.1 Application du Mod√®le K-means || :** Nous appliquons ce mod√®le sur les diff√©rentes repr√©sentations vectorielles (mots ou documents) afin de regrouper les propositions d'id√©es en clusters homog√®nes. Chaque cluster repr√©sente id√©alement un groupe d‚Äôid√©es similaires correspondant √† une th√©matique d√©j√† connue.
Le nombre de clusters a √©t√© fix√© √† **k = 5** en coh√©rence avec le nombre de th√©matiques identifi√©es dans nos donn√©es.  


**3.2 S√©lection de la Meilleure Repr√©sentation Vectorielle avec K-means**: Nous utilisons deux m√©triques pour √©valuer la qualit√© des clusters form√©s par K-means :  
- **Silhouette Score** : Cette m√©trique mesure la **compacit√©** des clusters et la **distance entre eux**.  
- **Davies-Bouldin Index (DBI)** : Cet indice √©value la **s√©paration** et la **compacit√©** des clusters (inter-cluster et intra-cluster).


**3.3 Validation des Clusters et Analyse Th√©matique**: Une fois la meilleure repr√©sentation vectorielle identifi√©e √† l‚Äôaide des m√©triques pr√©c√©dentes, nous avons effectu√© une validation des clusters en analysant leur correspondance avec les th√©matiques connues :  

 - **Tableau de Contingence** : Nous avons construit un tableau de contingence entre les clusters g√©n√©r√©s par K-means et les th√©matiques pr√©existantes dans les donn√©es. Cet outil permet de visualiser la correspondance entre les clusters et les th√©matiques r√©v√©lant leur coh√©rence et homog√©n√©it√©.
 - **Homogeneity Score** : Cette m√©trique mesure dans quelle mesure chaque cluster ne contient que des donn√©es appartenant √† une seule th√©matique. Un score d'homog√©n√©it√© √©lev√© indique que les clusters sont bien s√©par√©s en termes de th√©matiques.
 - **Adjusted Rand Index (ARI)** : Cette m√©trique √©value la similarit√© entre les clusters g√©n√©r√©s et les th√©matiques attendues en tenant compte des co√Øncidences dues au hasard. Un ARI √©lev√© indique une meilleure correspondance entre les clusters et les th√©matiques.

Ces analyses compl√©mentaires ont permis de confirmer la qualit√© des clusters et de valider la pertinence de la repr√©sentation vectorielle choisie.

----------------------

### 4Ô∏è‚É£ S√©lection des Mots ou des Documents Repr√©sentatifs pour Chaque Cluster

Lors de l'analyse pr√©c√©dente nous avons constat√© que les clusters g√©n√©r√©s par K-means pr√©sentent une forte similarit√©. De plus quelle que soit la repr√©sentation vectorielle utilis√©e,les propositions d'id√©es issues des diff√©rentes th√©matiques montrent un chevauchement important.

Pour finaliser le projet et extraire les termes (mots ou phrases) les plus repr√©sentatifs apr√®s le clustering nous avons adopt√© deux approches compl√©mentaires :

**4.1 S√©lection des Mots Repr√©sentatifs**

Nous avons utilis√© Word Embedding avec Word2Vec et combin√© deux m√©thodes pour identifier les termes repr√©sentatifs :

- **Cosine Similarity** : Cette m√©thode mesure la proximit√© des mots avec le centro√Øde de chaque cluster garantissant qu'ils repr√©sentent bien le th√®me principal.
                      Cependant, lorsque les clusters sont tr√®s similaires certains mots peuvent se chevaucher entre plusieurs clusters.
 - **TF-IDF** : Cette technique pond√®re les mots en fonction de leur fr√©quence dans un cluster et de leur raret√© dans l'ensemble des donn√©es.
               Cela permet de g√©rer les termes redondants mais n'int√®gre pas directement leur proximit√© avec le centro√Øde.
 - **Combinaison des deux** : En combinant TF-IDF et Cosine Similarity nous maximisons la s√©paration entre les clusters. Cette approche permet de s√©lectionner des termes sp√©cifiques et align√©s avec le centro√Øde tout en r√©duisant le chevauchement. Cela am√©liore la pr√©cision et la pertinence des termes repr√©sentatifs pour chaque cluster.


**4.2 S√©lection des documents Repr√©sentatifs + extraction de termes pertiant**

Pour identifier les documents les plus repr√©sentatifs de chaque cluster et en extraire les informations cl√©s nous avons utilis√© un sentence embedding avec USE (Universal Sentence Encoder). Deux techniques compl√©mentaires ont √©t√© mises en ≈ìuvre :

**4.2.1 Identification des Documents Repr√©sentatifs** : 
 - Nous avons utilis√© la distance cosinus pour mesurer la similarit√© entre chaque document et le centro√Øde du cluster.
 - Les 5 documents les plus proches de chaque centro√Øde ont √©t√© s√©lectionn√©s comme les plus pertinents refl√©tant au mieux la th√©matique du cluster.

**4.2.1 Extraction des Mots-Cl√©s et R√©sum√© Automatique**
 - **Extraction des mots-cl√©s** : Pour extraire les mots cl√©s nous avons utilis√© la **biblioth√®que RAKE** (Rapid Automatic Keyword Extraction). Cette m√©thode permet d'identifier les termes les plus significatifs dans les documents s√©lectionn√©s pour chaque cluster. RAKE est particuli√®rement efficace pour extraire des mots cl√©s pertinents en se basant sur la fr√©quence et la distribution des termes dans le texte.
 - **R√©sum√© automatique** : Pour produire un r√©sum√© de deux lignes par document nous avons employ√© la **biblioth√®que Sumy** en s'appuyant sur l'algorithme de r√©sum√© LSA (Latent Semantic Analysis). Cet algorithme identifie les phrases les plus importantes en fonction des relations s√©mantiques entre elles et offrant une synth√®se concise et pertinente.




